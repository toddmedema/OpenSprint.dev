{
  "version": 3,
  "sections": {
    "executive_summary": {
      "content": "OpenSprint is a web application that guides users through the complete software development lifecycle using AI agents. It provides a structured, four-phase workflow — Spec, Plan, Execute, and Examine — that transforms high-level product ideas into working software with minimal manual intervention.\n\nThe platform pairs a browser-based interface with a background agent CLI, enabling AI to autonomously execute development tasks while keeping the user in control of strategy and direction. The core philosophy is that humans should focus on _what_ to build and _why_, while AI handles _how_ to build it.\n\nOpenSprint supports multiple agent backends (Claude, Cursor, and custom CLI agents), comprehensive automated testing including end-to-end and integration tests, configurable human-in-the-loop thresholds, and full offline operation for users with local agent setups.",
      "version": 2,
      "updatedAt": "2026-02-17T07:33:18.682Z"
    },
    "problem_statement": {
      "content": "Building software with AI today is fragmented and unstructured. Developers use AI coding assistants for individual tasks, but there is no cohesive system that manages the full journey from idea to deployed product. This leads to several persistent problems:\n\n- **Lack of architectural coherence:** AI-generated code often lacks a unified vision because each prompt is handled in isolation, without awareness of the broader system design.\n- **No dependency tracking:** When building features in parallel, there is no mechanism to ensure that work on one feature accounts for dependencies on another.\n- **Manual orchestration overhead:** Users spend significant time managing prompts, context windows, and task sequencing rather than focusing on product decisions.\n- **No feedback loop:** There is no structured way to verify completed work and feed findings back into the development process.\n\nOpenSprint solves these problems by providing an end-to-end platform that maintains context across the entire lifecycle and automates the orchestration of AI development agents.\n\n",
      "version": 1,
      "updatedAt": "2026-02-15T09:08:59.834Z"
    },
    "goals_and_metrics": {
      "content": "### 3.1 Primary Goals\n\n1. Reduce the time from idea to working prototype by 10x compared to traditional AI-assisted development workflows.\n2. Enable non-engineers to ship production-quality software by handling technical complexity behind the scenes.\n3. Maintain architectural coherence across an entire project by flowing design decisions through every phase.\n4. Create a self-improving development flywheel where validation feedback automatically triggers corrective action.\n\n### 3.2 Success Metrics\n\n| Metric                              | Target                                     | Measurement Method               |\n| ----------------------------------- | ------------------------------------------ | -------------------------------- |\n| Time from idea to working prototype | < 1 day for standard web apps              | End-to-end session timing        |\n| User intervention rate during Execute | < 10% of tasks require manual input        | Task completion telemetry       |\n| Spec-to-code fidelity             | > 90% alignment with PRD                   | Automated PRD compliance checks  |\n| Feedback loop closure time          | < 30 min from bug report to fix deployed   | Examine-to-Execute cycle tracking |\n| First-time user task completion     | > 80% complete a full Spec-Execute cycle   | Onboarding funnel analytics      |\n| Test coverage                       | > 80% code coverage with passing E2E tests | Automated coverage reporting     |",
      "version": 2,
      "updatedAt": "2026-02-17T07:33:18.682Z"
    },
    "user_personas": {
      "content": "### 4.1 The Product-Minded Founder\n\nA non-technical founder with a clear product vision who wants to build an MVP without hiring a development team. They understand what they want to build but need AI to handle the engineering. They value speed, clear communication about what is being built, and the ability to provide feedback without writing code.\n\n### 4.2 The Solo Developer\n\nAn experienced developer who wants to multiply their output. They can code but want to delegate routine implementation to AI while focusing on architecture and product decisions. They value transparency into what the AI is doing, the ability to intervene when needed, and high-quality code output.\n\n### 4.3 The Agency / Consultancy\n\nA small team that builds software for clients. They need to move quickly from client requirements to working software, maintain multiple projects simultaneously, and provide clients with visibility into progress. They value the structured workflow for client communication and the ability to run multiple projects in parallel.\n\n",
      "version": 1,
      "updatedAt": "2026-02-15T09:08:59.834Z"
    },
    "technical_architecture": {
      "content": "### 5.6 Data Flow (addition)\n\n**Planning run storage:** When the user clicks \"Plan it\" or \"Replan it\", the orchestrator creates a planning run record in `.opensprint/planning-runs/` with a PRD snapshot. This supports the Replan flow: the planning agent receives the current PRD and the last run’s snapshot to compute a diff and propose create/update/archive operations for plans.",
      "version": 3,
      "updatedAt": "2026-02-17T07:49:15.826Z"
    },
    "feature_list": {
      "content": "#### 7.1.5 User Interface\n\nThe Dream tab presents a split-pane interface. The left pane is a chat window where the user converses with the planning agent. The right pane displays the live PRD document, updating in real-time as the conversation progresses.\n\n**Inline editing:** Users edit PRD sections directly inline — there is no Edit/Save flow. The PRD behaves like a live document (similar to Google Docs): users click into any section and type. A WYSIWYG-style editor supports rich formatting (e.g., Ctrl+B for bold) while storing content as markdown in the backend. Changes are saved automatically with debounced autosave per section. Users can click on any section to focus the conversation on that area; edits are reflected back into the conversation context.\n\n**Plan it / Replan it CTA:** A call-to-action button appears in the Dream phase header to move from PRD to Plan:\n- **Plan it** — Shown when no planning run exists. Clicking it runs the first decomposition of the PRD into feature plans.\n- **Replan it** — Shown when a planning run exists and the PRD has changed since the last run. Clicking it invokes the planning agent to review the PRD changes and create or update plans accordingly.\n- **Hidden** — Shown when a planning run exists and the PRD has not changed since the last run.\n\nThe button is disabled while a plan or replan operation is in progress. The frontend fetches plan-status on Dream load and after each PRD save to determine which label to show or whether to hide the button.\n\n#### 7.2.2 Key Capabilities (additions)\n\n- **Plan it / Replan it lifecycle:** The Dream phase CTA drives when decomposition runs. Before the first \"Plan it\", PRD edits are applied when the user clicks \"Plan it\". After the first run, any further PRD edits cause the button to show \"Replan it\". Clicking \"Replan it\" invokes the planning agent with the current PRD and the last planning run’s PRD snapshot; the agent diffs the changes and returns create/update/archive operations for plans. The orchestrator applies these changes and persists a new planning run with an updated PRD snapshot. If plans exist and the PRD is unchanged, the CTA is hidden.",
      "version": 3,
      "updatedAt": "2026-02-17T07:49:15.826Z"
    },
    "data_model": {
      "content": "#### PlanningRun\n\nStored in `.opensprint/planning-runs/<run-id>.json`. Each planning run records when decomposition or replan was executed and stores a PRD snapshot for diffing on Replan:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | string (UUID) | Unique run identifier |\n| created_at | datetime | When the run completed |\n| prd_snapshot | object | Full PRD state at run time (same structure as `prd.json`) |\n| plans_created | string[] | Plan IDs created or updated in this run |\n\nThe current working PRD remains in `.opensprint/prd.json`. Planning run snapshots are used only for Replan diffs; the agent receives the current PRD and the latest run’s snapshot to compute changes.",
      "version": 3,
      "updatedAt": "2026-02-17T07:49:15.826Z"
    },
    "api_contracts": {
      "content": "#### Plan Status\n\n| Method | Endpoint | Description |\n| ------ | -------- | ----------- |\n| GET | `/projects/:id/plan-status` | Returns `{ hasPlanningRun, prdChangedSinceLastRun, action }` where `action` is `\"plan\"` (show Plan it), `\"replan\"` (show Replan it), or `\"none\"` (hide button). Used by the Dream phase to drive CTA visibility.",
      "version": 3,
      "updatedAt": "2026-02-17T07:49:15.826Z"
    },
    "non_functional_requirements": {
      "content": "### 8.1 Philosophy\n\nOpenSprint takes an aggressive approach to automated testing. Every task completed by an AI agent must be accompanied by comprehensive tests. Testing is not optional or best-effort — it is a core requirement of task completion. A task is not considered Done until its tests pass.\n\n### 8.2 Testing Layers\n\n| Layer             | Scope                                                                     | When Generated                                            | When Run                                            |\n| ----------------- | ------------------------------------------------------------------------- | --------------------------------------------------------- | --------------------------------------------------- |\n| Unit Tests        | Individual functions and components                                       | Created by the agent as part of each task                 | On task completion; on every subsequent code change |\n| Integration Tests | Interactions between modules, API contracts, data flow between components | Created when a task involves multi-component interaction  | After dependent tasks complete; on every build      |\n| End-to-End Tests  | Full user flows through the application, simulating real user behavior    | Created per Plan epic once all tasks in the epic are Done | After epic completion; on every deployment         |\n| Regression Tests  | Ensure that fixes from Examine do not break existing functionality        | Auto-generated when an Examine ticket is resolved         | On every subsequent build                           |\n\n### 8.3 Test Execution\n\nThe test runner is configurable during project setup. OpenSprint supports common testing frameworks (Jest, Vitest, Playwright, Cypress, pytest, etc.) and will detect or recommend the appropriate framework based on the project's tech stack. Test results are displayed in the Execute tab alongside task status. Failed tests block a task from moving to Done and trigger either an automatic retry or escalation based on the Human-in-the-Loop configuration.\n\n### 8.4 Coverage Requirements\n\nOpenSprint targets a minimum of 80% code coverage across all generated code. Coverage reports are generated after each Execute cycle and displayed in the project dashboard. The AI agent is instructed to prioritize testing edge cases and error handling paths identified in the Plan markdown, not just happy paths.\n\n## 15. Cross-Cutting Concerns\n\n### 15.1 Living PRD Synchronization\n\nThe living PRD is the backbone of OpenSprint. Changes propagate to the PRD at two trigger points: (1) when a Plan is approved for execute, the planning agent reviews the Plan against the PRD and updates affected sections; (2) when Examine feedback is categorized as a scope change, the planning agent reviews the feedback and proposes PRD updates (subject to HIL approval). Both invocations use the same agent calling system as all other agent interactions. All PRD changes are recorded in the `change_log` with source attribution (which phase triggered the change) and full diff history. Users can view any historical version of the PRD in the Spec tab.\n\n### 15.2 Agent Orchestration\n\nThe agent orchestration layer manages the lifecycle of agent instances. All agents — planning, coding, and review — are invoked through the same mechanism: the user-configured agent API or CLI for that mode (see Section 6.3). The orchestrator runs a single agent at a time (v1), handling: task selection via `bd ready`, agent assignment via beads' `assignee` field, context assembly (gathering PRD sections, Plan markdowns, and outputs from dependency tasks into the task directory), the two-agent coding/review cycle, branch creation and cleanup, 5-minute inactivity timeout monitoring, and retry logic for failed tasks.\n\n### 15.3 Work Provenance (Beads)\n\nEvery piece of work in OpenSprint is traceable. The beads system captures: which design decision led to a feature, which Plan markdown specified the feature, which tasks implemented it, what the agent reasoned during implementation (via agent session logs), and what feedback was received post-execute. Users can query this provenance at any time by asking \"Why was this built this way?\" and receive a full trace from design decision to deployed code. Beads' built-in audit trail and `discovered-from` dependency type support this traceability natively.\n\n## 16. Non-Functional Requirements\n\n| Category        | Requirement                                                                                                  |\n| --------------- | ------------------------------------------------------------------------------------------------------------ |\n| Performance     | Real-time agent output streaming with < 500ms latency; kanban board updates within 1 second of state changes |\n| Scalability     | Handle projects with up to 500 tasks; single-agent execution in v1, concurrent agents planned for v2         |\n| Reliability     | Agent failures must not corrupt project state; all state changes are transactional and recoverable           |\n| Security        | Code execution in sandboxed environments; user projects isolated at the filesystem level                   |\n| Usability       | First-time users can create a Spec and reach Execute phase within 30 minutes without documentation           |\n| Data Integrity  | Full audit trail of every change via PRD versioning and bead provenance; no data loss on agent crash        |\n| Testing         | Minimum 80% code coverage; all test layers automated; test results visible in real-time                      |\n| Offline Support | All core features (Spec, Plan, Execute, Examine) fully functional without internet connectivity             |",
      "version": 2,
      "updatedAt": "2026-02-17T07:33:18.682Z"
    },
    "open_questions": {
      "content": "| Phase | Scope                                                                                                                                                                                                                                                                                                                     |\n| ----- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Alpha | Spec + Plan phases with living PRD; chat interface with planning agent; Plan markdown generation; agent selection during setup; project home screen                                                                                                                                                                     |\n| Beta  | Execute phase with kanban board, single-agent task execution with coding/review cycle, beads integration, agent CLI contract, unit test generation, and error handling                                                                                                                                                      |\n| v1.0  | Full Execute phase with real-time monitoring, comprehensive testing (unit + integration + E2E), HIL configuration, and 5-minute timeout handling                                                                                                                                                                            |\n| v1.1  | Examine phase with feedback ingestion, intelligent mapping, flywheel closure, and Expo.dev deployment integration                                                                                                                                                                                                        |\n| v2.0  | Concurrent multi-agent Execute execution with conflict resolution, conductor agent for context summarization, **Agent Dashboard tab** (view, monitor, and manage all agent status and output including conductor), multi-project support, team collaboration, custom deployment pipelines, regression test suite management |",
      "version": 2,
      "updatedAt": "2026-02-17T07:33:18.682Z"
    }
  },
  "changeLog": [
    {
      "section": "executive_summary",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-2 lines, +0 chars]"
    },
    {
      "section": "goals_and_metrics",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-2 lines, +2 chars]"
    },
    {
      "section": "technical_architecture",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-8 lines, +15 chars]"
    },
    {
      "section": "feature_list",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-8 lines, +47 chars]"
    },
    {
      "section": "data_model",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-2 lines, +6 chars]"
    },
    {
      "section": "api_contracts",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-251 lines, -11907 chars]"
    },
    {
      "section": "non_functional_requirements",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-44 lines, -2178 chars]"
    },
    {
      "section": "open_questions",
      "version": 2,
      "source": "plan",
      "timestamp": "2026-02-17T07:33:18.682Z",
      "diff": "[-51 lines, -9048 chars]"
    },
    {
      "section": "feature_list",
      "version": 3,
      "source": "plan",
      "timestamp": "2026-02-17T07:49:15.826Z",
      "diff": "[-105 lines, -14198 chars]"
    },
    {
      "section": "data_model",
      "version": 3,
      "source": "plan",
      "timestamp": "2026-02-17T07:49:15.826Z",
      "diff": "[-144 lines, -6573 chars]"
    },
    {
      "section": "api_contracts",
      "version": 3,
      "source": "plan",
      "timestamp": "2026-02-17T07:49:15.826Z",
      "diff": "[-84 lines, -5155 chars]"
    },
    {
      "section": "technical_architecture",
      "version": 3,
      "source": "plan",
      "timestamp": "2026-02-17T07:49:15.826Z",
      "diff": "[-159 lines, -17639 chars]"
    }
  ]
}